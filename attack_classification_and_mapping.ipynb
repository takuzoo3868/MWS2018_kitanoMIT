{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, sklearn, argparse, numpy as np, pandas as pd, tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "from keras.backend import tensorflow_backend\n",
    "import discriminator, geoip2.database, folium\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_kddi_data(file_path, multi_class):\n",
    "    \"\"\" Load KDDI Cup 99 Data\n",
    "    \"\"\"\n",
    "    col = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\",\n",
    "           \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\",\n",
    "           \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "           \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\",\n",
    "           \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "           \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "           \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
    "           \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "    dataset = pd.read_csv(file_path, names=col)\n",
    "    \n",
    "    labels = dataset[\"label\"]\n",
    "    if multi_class:\n",
    "        # Replace label to Benign: 0, Probe: 1, DoS: 2, U2R: 3, R2L: 4\n",
    "        # Benign: 通常のコネクション, Probe: 攻撃対象の探索・調査, DoS: DoS攻撃\n",
    "        # U2R: ローカルマシンからrootへの許可されていないアクセス\n",
    "        # R2L: リモートマシンからの許可されていないアクセス\n",
    "        labels = labels.replace({\"^.*normal.*\":0,\"^.*ipsweep.*\":1,\"^.*nmap.*\":1, \"^.*portsweep.*\":1,\n",
    "                                 \"^.*satan.*\":1,\"^.*mscan.*\":1,\"^.*saint.*\":1,\"^.*back.*\":2,\"^.*land.*\":2,\n",
    "                                 \"^.*neptune.*\":2,\"^.*pod.*\":2,\"^.*smurf.*\":2,\"^.*teardrop.*\":2,\n",
    "                                 \"^.*mailbomb.*\":2,\"^.*apache2.*\":2,\"^.*processtable.*\":2,\"^.*udpstorm.*\":2,\n",
    "                                 \"^.*buffer_overflow.*\":3,\"^.*loadmodule.*\":3,\"^.*perl.*\":3,\"^.*rootkit.*\":3,\n",
    "                                 \"^.*httptunnel.*\":3,\"^.*xterm.*\":3,\"^.*ps.*\":3,\"^.*worm.*\":3,\n",
    "                                 \"^.*ftp_write.*\":4,\"^.*guess_passwd.*\":4,\"^.*imap.*\":4,\"^.*multihop.*\":4,\n",
    "                                 \"^.*phf.*\":4,\"^.*spy.*\":4,\"^.*warezclient.*\":4,\"^.*warezmaster.*\":4,\n",
    "                                 \"^.*snmpgetattack.*\":4,\"^.*snmpguess.*\":4,\"^.*xsnoop.*\":4,\n",
    "                                 \"^.*named.*\":4,\"^.*sendmail.*\":4,\"^.*sqlattack.*\":4,\"^.*xlock.*\":4}, regex=True)\n",
    "    else:\n",
    "        # Replace label to Benign: 0, Malicious: 1\n",
    "        labels = labels.replace({\"^.*normal.*\":0,\"^(?!normal).*$\":1}, regex=True)\n",
    "    \n",
    "    # Drop columns of string and label\n",
    "    drop_columns = [\"protocol_type\", \"service\", \"flag\", \"label\"]\n",
    "    return dataset.drop(drop_columns, axis=1), labels\n",
    "\n",
    "\n",
    "def load_converted_data(file_path):\n",
    "    \"\"\" Load converted pcap data\n",
    "    \"\"\"\n",
    "    col = [\"num_conn\", \"startTimet\", \"orig_pt\", \"resp_pt\", \"orig_ht\", \"resp_ht\",\n",
    "           \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\",\n",
    "           \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\",\n",
    "           \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "           \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\",\n",
    "           \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "           \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "           \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
    "           \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
    "    dataset = pd.read_csv(file_path, names=col)\n",
    "    \n",
    "    # orig_ht: 送信元ip, resp_ht: 送信先ip\n",
    "    orig_ip_list = dataset[\"orig_ht\"]\n",
    "    resp_ip_list = dataset[\"resp_ht\"]\n",
    "    drop_columns = [\"num_conn\", \"startTimet\", \"orig_pt\", \"resp_pt\", \"orig_ht\", \"resp_ht\",\n",
    "                    \"protocol_type\", \"service\", \"flag\"]\n",
    "    return dataset.drop(drop_columns, axis=1), orig_ip_list, resp_ip_list\n",
    "\n",
    "\n",
    "def train(multi_class, use_gpu):\n",
    "    \"\"\" Training from KDDI Cup 99 data \n",
    "    \"\"\"\n",
    "    if use_gpu:\n",
    "        # Set GPU\n",
    "        config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "        session = tf.Session(config=config)\n",
    "        tensorflow_backend.set_session(session)\n",
    "        \n",
    "    # Load KDDI Data\n",
    "    X_kddi, y_kddi = load_kddi_data(file_path='./kddcup99/kddcup.data_10_percent', multi_class=multi_class)\n",
    "    \n",
    "    # Preprocess for data\n",
    "    split_size = .4   # split 40% of the data for test\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  \n",
    "    X = scaler.fit_transform(X_kddi)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "    y = np.array(y_kddi.tolist())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size, random_state=123)\n",
    "\n",
    "    # If training is slow, please set SAMPLE_SIZE\n",
    "    SAMPLE_SIZE = 0\n",
    "    if SAMPLE_SIZE:\n",
    "        X_train = X_train[:int(SAMPLE_SIZE*(1-split_size))]\n",
    "        y_train = y_train[:int(SAMPLE_SIZE*(1-split_size))]\n",
    "        X_test = X_test[:int(SAMPLE_SIZE*split_size)]\n",
    "        y_test = y_test[:int(SAMPLE_SIZE*split_size)]\n",
    "    \n",
    "    if multi_class:\n",
    "        y_train = np_utils.to_categorical(y_train, 5)\n",
    "        y_test = np_utils.to_categorical(y_test, 5)\n",
    "        \n",
    "    # Train\n",
    "    batch_size = 128\n",
    "    epochs = 100\n",
    "    nn_type = 'Dense'\n",
    "    os.makedirs('save_data', exist_ok=True)\n",
    "    if multi_class:\n",
    "        save_name = 'save_data/'+nn_type+'_weights_multi.h5'\n",
    "    else:\n",
    "        save_name = 'save_data/'+nn_type+'_weights.h5'\n",
    "    base = discriminator.BasicModel(multi_class)\n",
    "    clf = base.build(input_shape=(38, 1), nn_type=nn_type, vat=True)\n",
    "    clf.train(X_train, X_test, y_train, y_test, batch_size=batch_size, epochs=epochs, early_stop=True)\n",
    "    clf.model.save_weights(save_name)\n",
    "        \n",
    "        \n",
    "def predict(multi_class, use_gpu, file_path):\n",
    "    if use_gpu:\n",
    "        # Set GPU\n",
    "        config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "        session = tf.Session(config=config)\n",
    "        tensorflow_backend.set_session(session)\n",
    "        \n",
    "    # Load converted pcap data\n",
    "    with open(file_path, mode='r') as f:\n",
    "        s = f.read().replace(' ', ',')\n",
    "    with open(file_path, mode='w') as f:\n",
    "        f.write(s)\n",
    "    X_converted, orig_ip_list, resp_ip_list = load_converted_data(file_path=file_path)\n",
    "\n",
    "    # Preprocess for data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "    X_test_nolabel = scaler.fit_transform(X_converted)\n",
    "    X_test_nolabel = np.reshape(X_test_nolabel, (X_test_nolabel.shape[0], X_test_nolabel.shape[1], 1))\n",
    "    \n",
    "    # Predict\n",
    "    nn_type = 'Dense'\n",
    "    if multi_class:\n",
    "        save_name = 'save_data/'+nn_type+'_weights_multi.h5'\n",
    "    else:\n",
    "        save_name = 'save_data/'+nn_type+'_weights.h5'\n",
    "    base = discriminator.BasicModel(multi_class)\n",
    "    clf = base.build(input_shape=(38, 1), nn_type=nn_type, vat=True)\n",
    "    clf.model.load_weights(save_name)\n",
    "    predict_resoult = clf.model.predict(X_test_nolabel)\n",
    "    p_resoult = []\n",
    "    for i in range(len(predict_resoult)):\n",
    "        if multi_class:\n",
    "            p_resoult.append(predict_resoult[i].argmax())  \n",
    "        else:\n",
    "            for j in range(len(predict_resoult[i])):                                             \n",
    "                p_resoult.append(int(np.round(predict_resoult[i][j])))\n",
    "    \n",
    "    # Count appered ip address\n",
    "    connection_list = []\n",
    "    temp = []\n",
    "    appered_counta = []\n",
    "    for i in range(len(orig_ip_list)):\n",
    "        connection_list.append([orig_ip_list[i], resp_ip_list[i], p_resoult[i]])\n",
    "    for i in connection_list: \n",
    "        if not i in temp:\n",
    "            temp.append(i)\n",
    "            appered_counta.append([i, 1])\n",
    "        else:\n",
    "            for j in range(len(appered_counta)):\n",
    "                if appered_counta[j][0] == i:\n",
    "                    appered_counta[j][1]+=1;\n",
    "                    break\n",
    "                    \n",
    "    # Search ip info\n",
    "    orig_record = []\n",
    "    resp_record = []\n",
    "    for i in range(len(temp)):\n",
    "        orig_record.append(search_ip_info(ip=appered_counta[i][0][0]))\n",
    "        resp_record.append(search_ip_info(ip=appered_counta[i][0][1]))\n",
    "    # Print originator and responder\n",
    "    attack_type_multi = ['Benign', 'Probe', 'DoS', 'U2R', 'R2L']\n",
    "    attack_type = ['Benign', 'Malisious']\n",
    "    for i in range(len(orig_record)):\n",
    "        try:\n",
    "            print('orig: '+appered_counta[i][0][0]+'\\t'+orig_record[i].city.name, end=\"\\t\")\n",
    "            try:\n",
    "                print('resp: '+appered_counta[i][0][1]+'\\t'+resp_record[i].city.name, end=\"\\t\")\n",
    "            except:\n",
    "                print('resp: '+appered_counta[i][0][1]+'\\t'+'Private IP', end=\"\\t\")\n",
    "            if multi_class:\n",
    "                print('type: '+attack_type_multi[appered_counta[i][0][2]]+'\\t'+str(appered_counta[i][1])+' times')\n",
    "            else:\n",
    "                 print('type: '+attack_type[appered_counta[i][0][2]]+'\\t'+str(appered_counta[i][1])+' times')\n",
    "        except:\n",
    "            print('orig: '+appered_counta[i][0][0]+'\\t'+'Private IP', end=\"\\t\")\n",
    "            try:\n",
    "                print('resp: '+appered_counta[i][0][1]+'\\t'+resp_record[i].city.name, end=\"\\t\")\n",
    "            except:\n",
    "                print('resp: '+appered_counta[i][0][1]+'\\t'+'Private IP', end=\"\\t\")\n",
    "            if multi_class:\n",
    "                print('type: '+attack_type_multi[appered_counta[i][0][2]]+'\\t'+str(appered_counta[i][1])+' times')\n",
    "            else:\n",
    "                 print('type: '+attack_type[appered_counta[i][0][2]]+'\\t'+str(appered_counta[i][1])+' times')\n",
    "            \n",
    "    # Make ip map\n",
    "    make_map(multi_class=multi_class, appered_counta=appered_counta,\n",
    "             orig_record=orig_record, resp_record=resp_record, file_path=file_path)\n",
    "    \n",
    "\n",
    "def search_ip_info(ip):\n",
    "    # Load geoip database\n",
    "    reader = geoip2.database.Reader('./Geoip/GeoLite2-City.mmdb')    \n",
    "    try:\n",
    "        return reader.city(ip)\n",
    "    except:\n",
    "        return 'Private IP'\n",
    "\n",
    "\n",
    "def make_map(multi_class, appered_counta, orig_record, resp_record, file_path):\n",
    "    WEIGHT = 10\n",
    "    # Malicious: '#dc143c', Probe: '#0000ff', DoS: '#008000', U2R: '#ffa500', R2L: '#ee82ee'\n",
    "    color_list = ['#0000ff', '#008000', '#ffa500', '#ee82ee', 'dc143c']\n",
    "    \n",
    "    ip_map = folium.Map(location=[50, 8], zoom_start=4)#[30, 0], zoom_start=3)\n",
    "    for i in range(len(appered_counta)):\n",
    "        if multi_class:\n",
    "            if not appered_counta[i][0][2] == 0:\n",
    "                try:\n",
    "                    folium.vector_layers.CircleMarker(\n",
    "                        location=[resp_record[i].location.latitude, resp_record[i].location.longitude],\n",
    "                        popup=resp_record[i].city.name+'_resp by_'+appered_counta[i][0][0],\n",
    "                        radius=appered_counta[i][1]*WEIGHT,\n",
    "                        color=color_list[appered_counta[i][0][2]], fill_color=color_list[appered_counta[i][0][2]]\n",
    "                    ).add_to(ip_map)\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            if not appered_counta[i][0][2] == 0:\n",
    "                try:\n",
    "                    folium.vector_layers.CircleMarker(\n",
    "                        location=[resp_record[i].location.latitude, resp_record[i].location.longitude],\n",
    "                        popup=resp_record[i].city.name+'_resp by_'+appered_counta[i][0][0],\n",
    "                        radius=appered_counta[i][1]*WEIGHT,\n",
    "                        color=color_list[0], fill_color=color_list[appered_counta[i][0][2]]\n",
    "                    ).add_to(ip_map)\n",
    "                except:\n",
    "                    pass\n",
    "    ip_map.save(os.path.splitext(file_path)[0]+'-map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kawaiM/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 296412 samples, validate on 197609 samples\n",
      "Epoch 1/100\n",
      "296412/296412 [==============================] - 34s 114us/step - loss: 0.0739 - acc: 0.9781 - val_loss: 0.0454 - val_acc: 0.9886\n",
      "Epoch 2/100\n",
      "296412/296412 [==============================] - 32s 109us/step - loss: 0.0438 - acc: 0.9876 - val_loss: 0.0452 - val_acc: 0.9916\n",
      "Epoch 3/100\n",
      "296412/296412 [==============================] - 33s 111us/step - loss: 0.0480 - acc: 0.9897 - val_loss: 0.0399 - val_acc: 0.9941\n",
      "Epoch 4/100\n",
      "296412/296412 [==============================] - 33s 112us/step - loss: 0.0475 - acc: 0.9908 - val_loss: 0.0362 - val_acc: 0.9946\n",
      "Epoch 5/100\n",
      "296412/296412 [==============================] - 32s 109us/step - loss: 0.0471 - acc: 0.9909 - val_loss: 0.0376 - val_acc: 0.9946\n",
      "Epoch 6/100\n",
      "296412/296412 [==============================] - 32s 109us/step - loss: 0.0469 - acc: 0.9907 - val_loss: 0.0357 - val_acc: 0.9945\n",
      "Epoch 7/100\n",
      "296412/296412 [==============================] - 34s 116us/step - loss: 0.0473 - acc: 0.9909 - val_loss: 0.0359 - val_acc: 0.9946\n",
      "Epoch 8/100\n",
      "296412/296412 [==============================] - 32s 109us/step - loss: 0.0477 - acc: 0.9910 - val_loss: 0.0342 - val_acc: 0.9934\n",
      "Epoch 9/100\n",
      "296412/296412 [==============================] - 33s 112us/step - loss: 0.0469 - acc: 0.9910 - val_loss: 0.0364 - val_acc: 0.9925\n",
      "Epoch 10/100\n",
      "296412/296412 [==============================] - 33s 110us/step - loss: 0.0480 - acc: 0.9907 - val_loss: 0.0372 - val_acc: 0.9941\n",
      "Epoch 11/100\n",
      "296412/296412 [==============================] - 34s 116us/step - loss: 0.0471 - acc: 0.9907 - val_loss: 0.0376 - val_acc: 0.9947\n",
      "Epoch 12/100\n",
      "296412/296412 [==============================] - 33s 110us/step - loss: 0.0476 - acc: 0.9908 - val_loss: 0.0332 - val_acc: 0.9933\n",
      "Epoch 13/100\n",
      "296412/296412 [==============================] - 34s 114us/step - loss: 0.0476 - acc: 0.9909 - val_loss: 0.0389 - val_acc: 0.9932\n",
      "Epoch 14/100\n",
      "296412/296412 [==============================] - 33s 111us/step - loss: 0.0474 - acc: 0.9907 - val_loss: 0.0355 - val_acc: 0.9935\n",
      "Epoch 15/100\n",
      "296412/296412 [==============================] - 33s 110us/step - loss: 0.0482 - acc: 0.9907 - val_loss: 0.0358 - val_acc: 0.9934\n",
      "Epoch 16/100\n",
      "296412/296412 [==============================] - 32s 109us/step - loss: 0.0492 - acc: 0.9902 - val_loss: 0.0355 - val_acc: 0.9934\n",
      "Epoch 17/100\n",
      "296412/296412 [==============================] - 33s 110us/step - loss: 0.0486 - acc: 0.9904 - val_loss: 0.0363 - val_acc: 0.9937\n"
     ]
    }
   ],
   "source": [
    "train(multi_class=True, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kawaiM/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig: 10.3.153.4\tPrivate IP\tresp: 10.3.255.255\tPrivate IP\ttype: Benign\t10 times\n",
      "orig: 10.3.153.4\tPrivate IP\tresp: 10.2.149.1\tPrivate IP\ttype: Benign\t1 times\n",
      "orig: 10.3.153.4\tPrivate IP\tresp: 10.2.150.2\tPrivate IP\ttype: Benign\t2 times\n",
      "orig: 10.3.153.4\tPrivate IP\tresp: 10.2.149.4\tPrivate IP\ttype: Benign\t1 times\n",
      "orig: 10.3.153.4\tPrivate IP\tresp: 10.2.149.4\tPrivate IP\ttype: Malisious\t21 times\n",
      "orig: 10.3.126.1\tPrivate IP\tresp: 10.3.153.4\tPrivate IP\ttype: Benign\t2 times\n",
      "orig: 10.3.153.4\tPrivate IP\tresp: 10.1.147.1\tPrivate IP\ttype: Malisious\t4 times\n",
      "orig: 10.2.149.20\tPrivate IP\tresp: 10.3.153.4\tPrivate IP\ttype: Benign\t1 times\n",
      "orig: 10.2.149.20\tPrivate IP\tresp: 10.3.153.4\tPrivate IP\ttype: Malisious\t1 times\n",
      "orig: 10.3.153.4\tPrivate IP\tresp: 50.2.160.125\tFrankfurt am Main\ttype: Benign\t18 times\n",
      "orig: 10.3.153.20\tPrivate IP\tresp: 10.3.153.4\tPrivate IP\ttype: Malisious\t2 times\n",
      "orig: 10.3.153.4\tPrivate IP\tresp: 10.2.150.2\tPrivate IP\ttype: Malisious\t2 times\n",
      "orig: 10.3.126.1\tPrivate IP\tresp: 10.3.153.4\tPrivate IP\ttype: Malisious\t1 times\n"
     ]
    }
   ],
   "source": [
    "predict(multi_class=True, use_gpu=False, file_path='./trafAld.list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
